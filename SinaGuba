from bs4 import BeautifulSoup
from gensim import corpora,models
import jieba
import requests
import csv
symbol = 'sh000300'
base_url = 'http://guba.sina.com.cn/?s=bar&name=' + symbol + '&type=0&page='
datelist=[]
all_data=[]

csv_headers = ['date', 'symbol', 'content', 'TF-IDF']
f = csv.writer(open('SinaGuba.csv', 'w',  newline='\n'))
f.writerow(csv_headers)

def seg_sentence(sentence):
    words = jieba.lcut(sentence.strip())
    stopwordsFile = open('hit_stopwords.txt', 'r', encoding='UTF-8')
    stopwords=[]
    for line in stopwordsFile.readlines():
        stopwords.append(line)
    words_new = []
    for word in words:
        if word not in stopwords:
            words_new.append(word)
    return words_new

for page in range(1,2):
    web_url = base_url + str(page)
    r = requests.get(web_url)
    soup = BeautifulSoup(r.content, 'lxml')
    all_content = soup.find_all('table')[1].find_all('tr')
    for i in range(1, len(all_content)):
        topic = all_content[i].contents[5].text
        date = all_content[i].contents[9].text
        if date not in datelist:
            date_content_temp = []
            date_content_temp.append(date)
            date_content_temp.append(symbol)
            date_content_temp.append(topic.strip())
            all_data.append(date_content_temp)
            datelist.append(date)
        else:
            for item in all_data:
                if date in item:
                    item[2] = item[2] + topic
                    break

raw_documents = []
for item in all_data:
    raw_documents.append(item[2])
text = [[word for word in seg_sentence(document)] for document in raw_documents]
dictionary = corpora.Dictionary(text)
corpus = [dictionary.doc2bow(line) for line in text]
model = models.TfidfModel(corpus)
for item in all_data:
    item.append(model[dictionary.doc2bow(seg_sentence(item[2]))])
    f.writerow(item)

